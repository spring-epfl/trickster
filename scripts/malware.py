#!/usr/bin/env python3

# TODO: Update interface usage.

import sys

sys.path.append("..")

# Ignore warnings.
import warnings

warnings.filterwarnings("ignore")

# Handle library imports.
import pickle
import logging
import numpy as np
import pandas as pd
from os import listdir
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegressionCV
from scipy.sparse import csr_matrix, save_npz, load_npz, issparse
from tqdm import tqdm, trange

from trickster.search import a_star_search, ida_star_search
from trickster.adversarial_helper import *
from trickster.expansion import *

from defaultcontext import with_default_context
from profiled import Profiler, profiled

###########################################
###########################################
###########################################

# Handle global variables.
COUNTER_LIM = 50000
DEBUG_FREQ = 500

logger = None
SEED = 2018
np.random.seed(seed=SEED)

###########################################
###########################################
###########################################


class LogisticRegressionScikitSaliencyOracle:
    def __init__(self, model):
        self.model = model

    def eval(self, _):
        return self.model.coef_[0]


class DistortionBoundReachedError(Exception):
    pass


class ExampleDoesNotExistError(Exception):
    pass


# Functions that performs JSMA search.
@profiled
def find_adversarial_jsma(
    x,
    clf,
    oracle,
    transformable_feature_idxs,
    target_confidence=0.5,
    k=20,
    return_path=False,
):
    """
    Perform adversarial example search using Grosse et al. algorithm based on JSMA.
    """
    if clf.predict_proba([x])[0, 1] <= target_confidence:
        raise Exception("Initial example is already classified as bening.")
    if return_path:
        path = [x]

    x_star = np.array(x, dtype="float")
    distortions = 0

    while clf.predict_proba([x_star])[0, 1] > target_confidence and distortions < k:
        derivative = oracle.eval(x_star)
        idxs = np.argsort(derivative)

        for i, idx in enumerate(idxs):
            # Check if changing the feature is permitted.
            if x_star[idx] == 0 and idx in transformable_feature_idxs:
                x_star[idx] = 1
                if return_path:
                    path.append(np.array(x_star))
                break
            if i == len(idxs) - 1:
                e = "Adversarial example is impossible to create. Tried {} distortions.".format(
                    distortions
                )
                raise ExampleDoesNotExistError(e)

        distortions += 1

    if distortions == k:
        e = "Distortion bound {} reached.".format(k)
        raise DistortionBoundReachedError(e)

    if return_path:
        return x_star, distortions, path
    else:
        return x_star, distortions


###########################################
###########################################
###########################################

# Define experiment helper functions.
def load_transform_data_fn(data_file, **kwargs):
    """
    Load and preprocess data, returning the examples and labels as numpy.
    """
    # Try loading saved preprocessed data and classifier.
    with open(data_file, "rb") as f:
        obj = pickle.load(f)
        X, y = obj["X"], obj["y"]

    return X, y, None


def clf_fit_fn(X_train, y_train, data_file, **kwargs):
    # Try loading saved preprocessed data and classifier.
    with open(data_file, "rb") as f:
        obj = pickle.load(f)
        clf = obj["clf"]
    return clf


def get_expansions_fn(_, data_file, feat_count, feature_selection_seed, **kwargs):
    """
    Define expansions to perform on features and obtain feature indexes.
    """
    with open(data_file, "rb") as f:
        obj = pickle.load(f)
        label_encoder = obj["label_encoder"]

    features = np.array([c.split("::")[0] for c in label_encoder.classes_])

    # Find indexes of required features in the original feature space.
    idxs_provider = find_substring_occurences(features, "provider")
    idxs_permission = find_substring_occurences(features, "permission")
    idxs_activity = find_substring_occurences(features, "activity")
    idxs_service_receiver = find_substring_occurences(features, "service_receiver")
    idxs_intent = find_substring_occurences(features, "intent")

    # Concatenate indexes of transformable features.
    transformable_feature_idxs = idxs_provider + idxs_permission + idxs_activity
    transformable_feature_idxs += idxs_service_receiver + idxs_intent

    # Choose randomly features to perturb.
    np.random.seed(feature_selection_seed)
    transformable_feature_idxs = np.random.choice(
        transformable_feature_idxs, size=feat_count, replace=False
    )
    transformable_feature_idxs.sort()

    # Find indexes of required features in the reduced feature space.
    reduced_features = features[transformable_feature_idxs]
    reduced_transformable_feature_idxs = find_substring_occurences(
        reduced_features, "provider"
    )
    reduced_transformable_feature_idxs += find_substring_occurences(
        reduced_features, "permission"
    )
    reduced_transformable_feature_idxs += find_substring_occurences(
        reduced_features, "activity"
    )
    reduced_transformable_feature_idxs += find_substring_occurences(
        reduced_features, "service_receiver"
    )
    reduced_transformable_feature_idxs += find_substring_occurences(
        reduced_features, "intent"
    )

    # Set required expansions for features.
    expansions = [(reduced_transformable_feature_idxs, expand_collection_set)]

    return expansions, transformable_feature_idxs


def baseline_search_fn(
    X,
    idxs,
    clf,
    target_confidence,
    transformable_feature_idxs,
    p_norm,
    logger_name,
    **kwargs
):
    """Perform JSMA adversarial example search to baseline against A* search."""
    logger = logging.getLogger(logger_name)

    # Dataframe for storing the results.
    results = pd.DataFrame(
        columns=[
            "index",
            "found",
            "x",
            "init_confidence",
            "x_adv",
            "adv_confidence",
            "real_cost",
            "distortions",
            "optimal_path",
            "difference",
            "runtime",
        ]
    )

    # Oracle and distortion bound required by the JSMA algorithm.
    k, oracle = 20, LogisticRegressionScikitSaliencyOracle(clf)

    # Find adversarial examples using JSMA and record their costs.
    for i, idx in enumerate(tqdm(idxs, ascii=True)):

        logger.debug(
            "[JSMA] Searching for adversarial example {}/{} using initial observation at index: {}.".format(
                i, len(idxs), idx
            )
        )

        if issparse(X):
            x = X[idx].toarray()[0]
        else:
            x = X[idx]

        # Instantiate a profiler to analyse runtime.
        per_example_profiler = Profiler()

        x_adv, adv_found = None, None
        adv_confidence, difference = None, None
        real_cost, distortions = None, None
        runtime, optimal_path = None, None

        with per_example_profiler.as_default():
            try:
                x_adv, distortions, optimal_path = find_adversarial_jsma(
                    x=x,
                    clf=clf,
                    oracle=oracle,
                    transformable_feature_idxs=transformable_feature_idxs,
                    target_confidence=target_confidence,
                    k=k,
                    return_path=True,
                )
                adv_found = False if x_adv is None else True

            except (DistortionBoundReachedError, ExampleDoesNotExistError) as e:
                logger.debug(
                    "[JSMA] WARN! For observation at index {}: {}".format(idx, e)
                )

        # Record some basic statistics.
        init_confidence = clf.predict_proba([x])[0, 1]
        runtime_stats = per_example_profiler.compute_stats()
        if "find_adversarial" in runtime_stats:
            runtime = runtime_stats["find_adversarial"]["tot"]

        if x_adv is not None:
            logger.debug(
                "[JSMA] Adversarial example found {}/{} found using initial observation at index: {}!".format(
                    i, len(idxs), idx
                )
            )
            # Compute further statistics.
            adv_confidence = clf.predict_proba([x_adv])[0, 1]
            real_cost = np.linalg.norm(x - x_adv, ord=p_norm)
            difference, = np.where(x != x_adv)

        results.loc[i] = [
            idx,
            adv_found,
            x,
            init_confidence,
            x_adv,
            adv_confidence,
            real_cost,
            distortions,
            optimal_path,
            difference,
            runtime,
        ]

    return results


###########################################
###########################################
###########################################

# Main function.
if __name__ == "__main__":
    # Setup a custom logger.
    log_file = "../logging/malware_output.log"
    logger = setup_custom_logger(log_file)

    # Define debug parameters (set to None to disable).
    counter_lim = 1000000
    debug_freq = 10000

    # Define experiment parameters.
    data_file = "../scripts/tmp/preprocessed.pickle"
    target_confidence = 0.5
    confidence_margin = 0.35

    p_norm, q_norm = 1, np.inf
    feature_selection_iterations = 25
    feat_counts = np.arange(200, 49, -50)

    # Perform the experiments.
    logger.info("Starting experiments for the DREBIN malware dataset.")

    for feat_count in feat_counts:

        for i in range(feature_selection_iterations):

            output_file = "results/malware_{}_{}.pickle".format(feat_count, i)
            logger.info(
                "Experiment iteration {}/{} using {} features.".format(
                    i, feature_selection_iterations, feat_count
                )
            )

            result = experiment_wrapper(
                load_transform_data_fn=load_transform_data_fn,
                data_file=data_file,
                feat_count=feat_count,
                feature_selection_seed=SEED + i,
                p_norm=p_norm,
                q_norm=q_norm,
                clf_fit_fn=clf_fit_fn,
                get_expansions_fn=get_expansions_fn,
                expand_quantized_fn=expand_quantized,
                target_confidence=target_confidence,
                confidence_margin=confidence_margin,
                baseline_search_fn=baseline_search_fn,
                zero_to_one=False,
                random_state=SEED,
                counter_lim=counter_lim,
                debug_freq=debug_freq,
                logger=logger,
            )

            result["feature_count"] = feat_count
            result["feature_selection_iteration"] = i
            result["p_norm"] = p_norm
            result["q_norm"] = q_norm

            assert len(result["search_results"]) == len(result["baseline_results"])

            # Compare our approach with JSMA approach.
            N = len(result["search_results"])

            for j in range(N):
                astr_series = result["search_results"].loc[j]
                jsma_series = result["baseline_results"].loc[j]

                assert astr_series["index"] == jsma_series["index"]
                idx = astr_series["index"]

                if astr_series["real_cost"] != jsma_series["real_cost"]:
                    logger.info(
                        "Real cost differs for A* and JSMA for example at index: {}!".format(
                            idx
                        )
                    )
                if astr_series["path_cost"] != jsma_series["distortions"]:
                    logger.info(
                        "Path cost differs for A* and JSMA for example at index: {}!".format(
                            idx
                        )
                    )

            # Output results.
            logger.debug("Saving results to {}.".format(output_file))
            with open(output_file, "wb") as f:
                pickle.dump(result, f)
