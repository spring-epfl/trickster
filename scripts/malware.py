#!/usr/bin/env python3

import sys
sys.path.append('..')

# Ignore warnings.
import warnings
warnings.filterwarnings('ignore')

# Handle library imports.
import pickle
import logging
import numpy as np
import pandas as pd
from os import listdir
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegressionCV
from scipy.sparse import csr_matrix, save_npz, load_npz, issparse
from tqdm import tqdm, trange

from trickster.search import a_star_search, ida_star_search
from trickster.adversarial_helper import experiment_wrapper, substring_index, setup_custom_logger
from trickster.expansion import *

from defaultcontext import with_default_context
from profiled import Profiler, profiled

###########################################
###########################################
###########################################

# Handle global variables.

COUNTER_LIM = 50000
DEBUG_FREQ = 500


logger = None
SEED = 2018
np.random.seed(seed=SEED)

###########################################
###########################################
###########################################

class LogisticRegressionScikitSaliencyOracle:
    def __init__(self, model):
        self.model = model

    def eval(self, _):
        return self.model.coef_[0]

class DistortionBoundReachedError(Exception):
    pass

class ExampleDoesNotExistError(Exception):
    pass

# Functions that performs JSMA search.
@profiled
def find_adversarial_jsma(x, clf, oracle, transformable_feature_idxs, target_confidence=0.5, k=20, return_path=False):
    if clf.predict_proba([x])[0, 1] <= target_confidence:
        raise Exception('Initial example is already classified as bening.')
    if return_path:
        path = [x]

    x_star = np.array(x, dtype='intc')
    distortions = 0

    while clf.predict_proba([x_star])[0, 1] > target_confidence and distortions < k:
        derivative = oracle.eval(x_star)
        idxs = np.argsort(derivative)

        for i, idx in enumerate(idxs):
            # Check if changing the feature is permitted.
            if x_star[idx] == 0 and idx in transformable_feature_idxs:
                x_star[idx] = 1
                if return_path:
                    path.append(np.array(x_star))
                break
            if i == len(idxs) - 1:
                e = 'Adversarial example is impossible to create. Tried {} distortions.'.format(distortions)
                raise ExampleDoesNotExistError(e)

        distortions += 1

    if distortions == k:
        e = 'Distortion bound {} reached.'.format(k)
        raise DistortionBoundReachedError(e)

    if return_path:
        return x_star, distortions, path
    else:
        return x_star, distortions

###########################################
###########################################
###########################################

# Define experiment helper functions.
def load_transform_data_fn(data_file, **kwargs):
    '''Description goes here.'''
    # Try loading saved preprocessed data and classifier.
    with open(data_file, 'rb') as f:
        obj = pickle.load(f)
        X, y = obj['X'], obj['y']

    return X, y, None

def clf_fit_fn(X_train, y_train, data_file, **kwargs):
    # Try loading saved preprocessed data and classifier.
    with open(data_file, 'rb') as f:
        obj = pickle.load(f)
        clf = obj['clf']
    return clf

def get_expansions_fn(_, data_file, feat_count, **kwargs):
    '''Add description here.'''
    with open(data_file, 'rb') as f:
        obj = pickle.load(f)
        label_encoder = obj['label_encoder']

    features = np.array([c.split('::')[0] for c in label_encoder.classes_])

    # Find indexes of required features in the original feature space.
    idxs_provider = substring_index(features, 'provider')
    idxs_permission = substring_index(features, 'permission')
    idxs_activity = substring_index(features, 'activity')
    idxs_service_receiver = substring_index(features, 'service_receiver')
    idxs_intent = substring_index(features, 'intent')

    # Concatenate indexes of transformable features.
    transformable_feature_idxs = idxs_provider + idxs_permission + idxs_activity
    transformable_feature_idxs += idxs_service_receiver + idxs_intent

    # Choose randomly features to perturb.
    transformable_feature_idxs = np.random.choice(transformable_feature_idxs, size=feat_count, replace=False)
    transformable_feature_idxs.sort()

    # Find indexes of required features in the reduced feature space.
    reduced_features = features[transformable_feature_idxs]
    reduced_transformable_feature_idxs = substring_index(reduced_features, 'provider')
    reduced_transformable_feature_idxs += substring_index(reduced_features, 'permission')
    reduced_transformable_feature_idxs += substring_index(reduced_features, 'activity')
    reduced_transformable_feature_idxs += substring_index(reduced_features, 'service_receiver')
    reduced_transformable_feature_idxs += substring_index(reduced_features, 'intent')

    # Set required expansions for features.
    expansions = [
        (reduced_transformable_feature_idxs, expand_collection_set)
    ]

    return expansions, transformable_feature_idxs

def benchmark_search_fn(X, idxs, clf, target_confidence,
    transformable_feature_idxs, p_norm, logger_name, **kwargs):
    '''Perform JSMA adversarial example search to benchmark against A* search.'''
    logger = logging.getLogger(logger_name)

    # Dataframe for storing the results.
    results = pd.DataFrame(
        columns=['index', 'found', 'x', 'init_confidence', 'x_adv',
                 'adv_confidence', 'real_cost', 'distortions',
                 'difference', 'runtime']
    )

    # Oracle and distortion bound required by the JSMA algorithm.
    k, oracle = 20, LogisticRegressionScikitSaliencyOracle(clf)

    # Find adversarial examples using JSMA and record their costs.
    for i, idx in enumerate(tqdm(idxs, ascii=True)):

        logger.debug('[JSMA] Searching for adversarial example {}/{} using initial observation at index: {}.'
            .format(i, len(idxs), idx))

        if issparse(X):
            x = X[idx].toarray()[0]
        else:
            x = X[idx]

        # Instantiate a profiler to analyse runtime.
        per_example_profiler = Profiler()

        x_adv, adv_found = None, None
        adv_confidence, difference = None, None
        real_cost, distortions = None, None
        runtime = None

        with per_example_profiler.as_default():
            try:
                x_adv, distortions = find_adversarial_jsma(
                    x=x,
                    clf=clf,
                    oracle=oracle,
                    transformable_feature_idxs=transformable_feature_idxs,
                    target_confidence=target_confidence,
                    k=k
                )
                adv_found = False if x_adv is None else True

            except (DistortionBoundReachedError, ExampleDoesNotExistError) as e:
                logger.debug('[JSMA] WARN! For observation at index {}: {}'.format(idx, e))

        # Record some basic statistics.
        init_confidence = clf.predict_proba([x])[0, 1]
        runtime_stats = per_example_profiler.compute_stats()
        if 'find_adversarial' in runtime_stats:
            runtime = runtime_stats['find_adversarial']['tot']

        if x_adv is not None:
            logger.debug('[JSMA] Adversarial example found {}/{} found using initial observation at index: {}!'
                .format(i, len(idxs), idx))
            # Compute further statistics.
            adv_confidence = clf.predict_proba([x_adv])[0, 1]
            real_cost = np.linalg.norm(x - x_adv, ord=p_norm)
            difference, = np.where(x != x_adv)

        results.loc[i] = [
            idx, adv_found, x, init_confidence,
            x_adv, adv_confidence, real_cost, distortions,
            difference, runtime
        ]

    return results

###########################################
###########################################
###########################################

# Main function.
if __name__ == "__main__":
    # Setup a custom logger.
    log_file = '../logging/malware_output.log'
    logger = setup_custom_logger(log_file)

    # Define debug parameters.
    counter_lim = 100000
    debug_freq = 1000

    # Define experiment parameters.
    data_file = '../scripts/tmp/preprocessed.pickle'
    target_confidence=0.5
    confidence_margin=0.25

    # Perform the experiments.
    logger.info('Starting experiments for the DREBIN malware dataset.')

    result = experiment_wrapper(
        load_transform_data_fn=load_transform_data_fn,
        data_file=data_file,
        feat_count=50,
        clf_fit_fn=clf_fit_fn,
        get_expansions_fn=get_expansions_fn,
        expand_quantized_fn=expand_quantized,
        target_confidence=target_confidence,
        confidence_margin=confidence_margin,
        benchmark_search_fn=benchmark_search_fn,
        zero_to_one=False,
        random_state=SEED,
        counter_lim=counter_lim,
        debug_freq=debug_freq,
        logger=logger
    )

    # Compare our approach with JSMA approach.
    N = len(result['search_results'])

    for i in range(N):
        astr_series = result['search_results'].loc[i]
        jsma_series = result['benchmark_results'].loc[i]

        assert astr_series['index'] == jsma_series['index']
        idx = astr_series['index']

        if astr_series['real_cost'] != jsma_series['real_cost']:
            logger.info('Real cost differs for A* and JSMA for example at index: {}!'.format(idx))
        if astr_series['path_cost'] != jsma_series['distortions']:
            logger.info('Path cost differs for A* and JSMA for example at index: {}!'.format(idx))

    import pdb; pdb.set_trace()
